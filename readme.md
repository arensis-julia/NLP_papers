# PAPER SUMMARY
## BASICS
|이름|저자|년도|비고|링크|
|:---:|:---:|:---:|:---:|:---:|
|Attention is All You Need|A Vaswani et al.|2017|Transformer|[paper](https://arxiv.org/abs/1706.03762) \| [notion](https://www.notion.so/Attention-Is-All-You-Need-ec1f92159eee4c9a8fc9fd9dc07d7be9)
|Deep contextualized word representations|ME Peters et al.|2018|ELMo|[paper](https://arxiv.org/abs/1802.05365) \| [notion](https://www.notion.so/Deep-contextualized-word-representations-dcb94abb9c0a4d5d8014d40626ef837a) \| [allennlp](https://allennlp.org/elmo)
|Improving Language Understanding by Generative Pre-Training|A Radford et al.|2018|GPT|[paper](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) \| [notion](https://www.notion.so/Improving-Language-Understanding-by-Generative-Pre-Training-0b5ca43db12641d5a28b1060ffa99db4)
|BERT: Pre-training of Deep bidirectional Transformers for Language Understanding|J Devlin et al.|2019|BERT|[paper](https://arxiv.org/abs/1810.04805) \| [notion](https://www.notion.so/BERT-Pre-Training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-366ce4355f6842da9cf2e91e2b953e24) \| [github](https://github.com/google-research/bert)
|Language Models are Unsupervised Multitask Learners|A Radford et al.|2019|GPT2|[paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) \| [notion](https://www.notion.so/Language-Models-are-Unsupervised-Multitask-Learners-1d51509d50084d7e886c8c6580ada3d8) \| [github](https://github.com/openai/gpt-2)
|RoBERTa: A Robustly Optimized BERT Pretraining Approach|Y Liu et al.|2019|RoBERTa|[paper](https://arxiv.org/abs/1907.11692) \| [notion](https://glorious-cycle-ccb.notion.site/RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach-1e805221f45f419eac6cce36c896fe6e) \| [github](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md)
|ALBERT: A Lite BERT for Self-supervised Learning of Language Representations|Z Lan et al.|2019|ALBERT|[paper](https://arxiv.org/abs/1909.11942) \| [notion](https://glorious-cycle-ccb.notion.site/ALBERT-A-Lite-BERT-for-Self-supervised-Learning-of-Language-Representations-f9338e6630254f51aea46370f0902ffa) \| [github](https://github.com/google-research/ALBERT)
|BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension|M Lewis et al.|2019|BART|[paper](https://arxiv.org/abs/1910.13461) \| [notion](https://glorious-cycle-ccb.notion.site/BART-Denoising-Sequence-to-Sequence-Pre-training-for-Natural-Language-Generation-Translation-and--9168754bb6fd43e1889dadff44fe451e) \| [github](https://github.com/pytorch/fairseq/tree/master/examples/bart)
|ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators|K Clark et al.|2020|ELECTRA|[paper](https://arxiv.org/abs/2003.10555) \| [notion](https://glorious-cycle-ccb.notion.site/ELECTRA-Pre-training-Text-Encoders-as-Discriminators-Rather-Than-Generators-bae694ec771c41978196e509b192a9bf) \| [github](https://github.com/google-research/electra)
|Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer|C Raffel et al.|2020|T5|[paper](https://arxiv.org/abs/1910.10683) \| [notion](https://glorious-cycle-ccb.notion.site/Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer-497774bb21b346a4bc46564f96d4ffd3) \| [github](https://github.com/google-research/text-to-text-transfer-transformer)
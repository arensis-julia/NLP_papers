# PAPER SUMMARY
## BASICS
|이름|저자|년도|비고|링크|
|:---:|:---:|:---:|:---:|:---:|
|Attention is All You Need|A Vaswani et al.|2017|Transformer|[paper](https://arxiv.org/abs/1706.03762) \| [notion](https://www.notion.so/Attention-Is-All-You-Need-ec1f92159eee4c9a8fc9fd9dc07d7be9)
|Deep contextualized word representations|ME Peters et al.|2018|ELMo|[paper](https://arxiv.org/abs/1802.05365) \| [notion](https://www.notion.so/Deep-contextualized-word-representations-dcb94abb9c0a4d5d8014d40626ef837a) \| [allennlp](https://allennlp.org/elmo)
|Improving Language Understanding by Generative Pre-Training|A Radford et al.|2018|GPT|[paper](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) \| [notion](https://www.notion.so/Improving-Language-Understanding-by-Generative-Pre-Training-0b5ca43db12641d5a28b1060ffa99db4)
|BERT: Pre-training of Deep bidirectional Transformers for Language Understanding|J Devlin et al.|2019|BERT|[paper](https://arxiv.org/abs/1810.04805) \| [notion](https://www.notion.so/BERT-Pre-Training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-366ce4355f6842da9cf2e91e2b953e24) \| [github](https://github.com/google-research/bert)
|Language Models are Unsupervised Multitask Learners|A Radford et al.|2019|GPT2|[paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) \| [notion](https://www.notion.so/Language-Models-are-Unsupervised-Multitask-Learners-1d51509d50084d7e886c8c6580ada3d8) \| [github](https://github.com/openai/gpt-2)
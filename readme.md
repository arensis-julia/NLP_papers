# PAPER SUMMARY
## BASICS
|이름|저자|년도|비고|링크|
|:---:|:---:|:---:|:---:|:---:|
|Attention is All You Need|A Vaswani et al.|2017|Transformer|[paper](https://arxiv.org/abs/1706.03762) \| [notion](https://www.notion.so/Attention-Is-All-You-Need-ec1f92159eee4c9a8fc9fd9dc07d7be9)
|Deep contextualized word representations|ME Peters et al.|2018|ELMo|[paper](https://arxiv.org/abs/1802.05365) \| [notion](https://www.notion.so/Deep-contextualized-word-representations-dcb94abb9c0a4d5d8014d40626ef837a) \| [allennlp](https://allennlp.org/elmo)
|Improving Language Understanding by Generative Pre-Training|A Radford et al.|2018|GPT|[paper](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) \| [notion](https://www.notion.so/Improving-Language-Understanding-by-Generative-Pre-Training-0b5ca43db12641d5a28b1060ffa99db4)